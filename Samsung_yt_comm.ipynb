{"cells":[{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":4517,"status":"ok","timestamp":1721564717451,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"J6PEPHjt7oDi"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","from textblob import TextBlob\n","from collections import Counter\n","import seaborn as sns\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nlppreprocess import NLP\n","from nltk.util import ngrams"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":742,"status":"ok","timestamp":1721564718190,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"ku4JUJqcTPfP","outputId":"a2fdc85e-6cb9-4dbf-d2e8-fa7cfcf6f3f6"},"outputs":[],"source":["nltk.download('stopwords')\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721564718191,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"oxS3-s6U8Cf6"},"outputs":[],"source":["df = pd.read_csv('/Users/aryan/Coding/Projects/Samsung review project/Data/comments_data.csv')"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":602,"status":"ok","timestamp":1721564736880,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"GDAsEPsZ-ITt"},"outputs":[],"source":["df.dropna(inplace=True)"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":482,"status":"ok","timestamp":1721564738026,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"nzfB4FiIMwgW"},"outputs":[],"source":["nlp = NLP()\n","df['C_Comment'] = df['Comment'].apply(nlp.process)"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721564738027,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"SURXANniQf9z"},"outputs":[],"source":["\n","def remove_stop_words(text):\n","    # Get the list of English stop words\n","    stop_words = set(stopwords.words('english'))\n","\n","    # Tokenize the text\n","    word_tokens = text.split()\n","\n","    # Remove stop words\n","    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n","\n","    # Join the words back into a string\n","    return ' '.join(filtered_text)"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":490,"status":"ok","timestamp":1721564740547,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"KfFBM3GE9Mvf"},"outputs":[],"source":["# df['cleaned_comment'] = df['Comment'].apply(lambda x: preprocess_comment(x, preserve_words))\n","df['cleaned_comment'] = df['C_Comment'].apply(remove_stop_words)"]},{"cell_type":"code","execution_count":45,"metadata":{"collapsed":true,"executionInfo":{"elapsed":2960,"status":"ok","timestamp":1721564743506,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"QpKaFkjx_O0-"},"outputs":[],"source":["# Sentiment analysis\n","df['sentiment'] = df['cleaned_comment'].apply(lambda x: TextBlob(x).sentiment.polarity)\n","df['Polarity'] =  df['cleaned_comment'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n","df['sentiment_label'] = df['sentiment'].apply(lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral'))"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1721564743506,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"L-tWfKYrA_jR"},"outputs":[],"source":["df['words'] = df['cleaned_comment'].apply(lambda x: x.split())"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1721564744266,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"2Ltnw9QfPh6h"},"outputs":[],"source":["##Data Cleaning : removing some custom rows\n","dp_1 = df[df['cleaned_comment'].str.contains('ooh')].index.tolist()\n","dp_2 = df[df['Username'].str.contains('samgold9151')].index.tolist()\n","drop_index = dp_1 + dp_2\n","df.drop(drop_index,inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":871},"executionInfo":{"elapsed":2981,"status":"ok","timestamp":1721564747245,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"6-qlcH9uCLSI","outputId":"0cc4b66d-e40e-45d2-f3f8-254a8c11e473"},"outputs":[],"source":["# Get top words\n","positive_words = Counter([word for words in df[df['sentiment_label'] == 'positive']['words'] for word in words])\n","negative_words = Counter([word for words in df[df['sentiment_label'] == 'negative']['words'] for word in words])\n","\n","# Visualization\n","def plot_Wwords(word_counts, title):\n","    top_words = dict(word_counts.most_common(30))\n","    wordcloud = WordCloud(width=800, height=400).generate_from_frequencies(top_words)\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.title(title)\n","    plt.axis('off')\n","    plt.show()\n","\n","def plot_Bwords(words,title):\n","  words = dict(words)\n","  plt.figure(figsize=(10,5))\n","  sns.barplot(x=words.values(),y=words.keys())\n","  plt.title(title)\n","  plt.xlabel('Count')\n","  plt.show()\n","\n","\n","plot_Wwords(positive_words, 'Top 30 Positive Words')\n","plot_Wwords(negative_words, 'Top 30 Negative Words')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":957},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1721564747998,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"Z6XWK6ZHZShG","outputId":"21b7938d-cd14-4652-8d50-fc5e70412f72"},"outputs":[],"source":["plot_Bwords(positive_words.most_common(10),'Top 10  positive words')\n","plot_Bwords(negative_words.most_common(10),'Top 10  negative words')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":871},"executionInfo":{"elapsed":1800,"status":"ok","timestamp":1721564749796,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"SQT4NnR0BVnO","outputId":"1ee44fc5-b7f7-4513-fa69-8c11115cb4b4"},"outputs":[],"source":["#most common highly positive and negative words\n","high_pos = df[df['sentiment'] > 0.75]['words'].copy()\n","high_neg = df[df['sentiment'] < -0.75]['words'].copy()\n","high_pos_words = Counter([word for words in high_pos for word in words])\n","high_neg_words = Counter([word for words in high_neg for word in words])\n","\n","#Visualizing\n","plot_Wwords(high_pos_words,'Top 30 highly positive words')\n","plot_Wwords(high_neg_words,'Top 30 highly negative words')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":482,"status":"ok","timestamp":1721567128245,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"0R8-McYKVxvr","outputId":"d11f7b82-b90c-4ed1-852e-a06697a71a1a"},"outputs":[],"source":["high_pos_words.most_common(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":957},"executionInfo":{"elapsed":1015,"status":"ok","timestamp":1721564751906,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"4IiPA4nRZVkH","outputId":"45903ef9-bfca-41a5-f991-8ed4ef5cd794"},"outputs":[],"source":["plot_Bwords(high_pos_words.most_common(10),'Top 10 highly positive words')\n","plot_Bwords(high_neg_words.most_common(10),'Top 10 highly negative words')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":537,"status":"ok","timestamp":1721567428943,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"dN3MrbS0acv8","outputId":"9c74d1b9-35be-4d5c-c7f1-41bdfcea18dc"},"outputs":[],"source":["positive_bigrams.most_common(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":957},"executionInfo":{"elapsed":921,"status":"ok","timestamp":1721564752826,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"sMF8xYzHPvsk","outputId":"0040a5ed-1ceb-4c17-c5c5-7486b728a698"},"outputs":[],"source":["# Positive phrases\n","def get_bigrams(text):\n","    text = text.lower()\n","    text = text.split()\n","    return list(ngrams(text, 2))\n","\n","df['bigrams'] = df['cleaned_comment'].apply(get_bigrams)\n","\n","positive_bigrams = Counter([phrase for phrases in df[df['sentiment_label'] == 'positive']['bigrams'] for phrase in phrases])\n","negative_bigrams = Counter([phrase for phrases in df[df['sentiment_label'] == 'negative']['bigrams'] for phrase in phrases])\n","\n","#Visualizing\n","postivive_phrases = {','.join(phrases) : count for phrases,count in positive_bigrams.most_common(10)}\n","negative_phrases = {','.join(phrases) : count for phrases,count in negative_bigrams.most_common(10)}\n","\n","def plot_phrases(phrases,title):\n","  plt.figure(figsize=(10,5))\n","  sns.barplot(x=phrases.values(),y=phrases.keys())\n","  plt.title(title)\n","  plt.xlabel('Count')\n","  plt.show()\n","\n","plot_phrases(postivive_phrases,'Top 10 most common positive phraes')\n","plot_phrases(negative_phrases,'Top 10 most common negative phraes')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":1543,"status":"ok","timestamp":1721564759225,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"IbDIBv6WBKrh","outputId":"86b5bc18-a8d0-4370-d5d0-5a5360f202ea"},"outputs":[],"source":["# Length vs Sentiment\n","df['comment_length'] = df['cleaned_comment'].apply(len)\n","sns.kdeplot(x='comment_length', data=df,hue='sentiment_label',clip=(-10,300))\n","plt.title('Comment Length vs Sentiment')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"elapsed":3085,"status":"ok","timestamp":1721565206919,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"rg_pL8mWBVk6","outputId":"be6a872f-4e1a-485c-d6c1-415a3e103989"},"outputs":[],"source":["# Sentiment Distribution\n","sns.histplot(df['sentiment'], kde=True).set_yscale('log')\n","plt.title('Sentiment Score Distribution')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"executionInfo":{"elapsed":2047,"status":"ok","timestamp":1721564763875,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"slgauiEkrARl","outputId":"0d0a877c-d436-4146-c201-bfe7ff175617"},"outputs":[],"source":["sns.scatterplot(x='sentiment',y='Polarity',data=df,hue='sentiment_label')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1721566463144,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"FVrTHXd4W3lw","outputId":"4ad99e6a-aad7-4174-f782-42fae374c520"},"outputs":[],"source":["df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":2019},"executionInfo":{"elapsed":479,"status":"ok","timestamp":1721566719497,"user":{"displayName":"Aryan Kumar Singh","userId":"06092026006571645802"},"user_tz":-330},"id":"HRDmFSxuSBwk","outputId":"5d419df1-1c63-4be8-d631-e749ae1164b9"},"outputs":[],"source":["# Filter negative sentiment rows\n","negative_df = df[df['sentiment_label'] == 'negative']\n","\n","# Filter for phone mentions in the cleaned_comment column within the negative DataFrame\n","filtered_df = negative_df[negative_df['cleaned_comment'].str.contains('stupid')]\n","filtered_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1dN_WE5zWw6t"},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{},"source":["### Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","\n","comments = df['cleaned_comment'].values\n","labels = df['sentiment_label'].values\n","\n","# 2. Split Data\n","X_train, X_test, y_train, y_test = train_test_split(comments, labels, test_size=0.2, random_state=42) \n","\n","# 3. Feature Extraction with TF-IDF\n","vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')  # You can adjust max_features\n","X_train_vec = vectorizer.fit_transform(X_train)\n","X_test_vec = vectorizer.transform(X_test)\n","\n","# 4. Model Training (Logistic Regression)\n","model = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n","model.fit(X_train_vec, y_train)\n","\n","# 5. Prediction and Evaluation\n","y_pred = model.predict(X_test_vec)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Accuracy: {accuracy}\")\n","\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","\n","model = LogisticRegression(max_iter=1000) \n","cv_scores = cross_val_score(model, X_train_vec, y_train, cv=5)\n","\n","print(f\"Cross-validation scores: {cv_scores}\")\n","print(f\"Average CV accuracy: {cv_scores.mean()}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["(df['sentiment_label'].value_counts()/len(df))*100"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, classification_report\n","from imblearn.over_sampling import SMOTE \n","\n","\n","comments = df['cleaned_comment'].values\n","labels = df['sentiment_label'].values\n","\n","# 1. Split Data\n","X_train, X_test, y_train, y_test = train_test_split(comments, labels, test_size=0.2, random_state=42) \n","\n","# 2. Feature Extraction with TF-IDF\n","vectorizer = TfidfVectorizer(max_features=5000, stop_words='english') \n","X_train_vec = vectorizer.fit_transform(X_train)\n","X_test_vec = vectorizer.transform(X_test)\n","\n","# 3. Handle Class Imbalance (SMOTE)\n","smote = SMOTE(random_state=42)\n","X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vec, y_train)\n","\n","# 4. Model Training \n","\n","# Logistic Regression\n","model_lr = LogisticRegression(max_iter=1000, class_weight='balanced')  \n","model_lr.fit(X_train_resampled, y_train_resampled)\n","\n","# Decision Tree Classifier\n","model_dt = DecisionTreeClassifier(random_state=42)\n","model_dt.fit(X_train_resampled, y_train_resampled)\n","\n","# 5. Prediction and Evaluation\n","\n","# Logistic Regression\n","y_pred_lr = model_lr.predict(X_test_vec)\n","print(\"Logistic Regression:\")\n","print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr)}\")\n","print(classification_report(y_test, y_pred_lr))\n","\n","# Decision Tree\n","y_pred_dt = model_dt.predict(X_test_vec)\n","print(\"\\nDecision Tree:\")\n","print(f\"Accuracy: {accuracy_score(y_test, y_pred_dt)}\")\n","print(classification_report(y_test, y_pred_dt))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMtIk+jQEs+44fsT6fT0cQJ","provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
